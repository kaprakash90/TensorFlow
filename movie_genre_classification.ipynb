{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports for data processing\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import shuffle\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "class LabeledLineSentence():\n",
    "    def __init__(self, doc_list, label,categories):\n",
    "        self.label = label     \n",
    "        self.doc_list = doc_list\n",
    "        self.categories = categories\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            #print(self.label[idx][0])\n",
    "            #idx = self.labels.index()\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,    \n",
    "[self.categories[self.label[idx][0]]])\n",
    "            \n",
    "class SetData():\n",
    "    def __init__(self):\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.train_data_dir = '/Users/arunprakash/Documents/TensorFlow/MovieData/train'\n",
    "        self.test_data_dir = '/Users/arunprakash/Documents/TensorFlow/MovieData/test'\n",
    "        for _, dirnames, _ in os.walk(self.train_data_dir):\n",
    "            self.num_classes += len(dirnames)\n",
    "            if len(dirnames)>0:\n",
    "                self.classes = dirnames\n",
    "        self.stemmer = LancasterStemmer()\n",
    "        self.tbl = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    def one_hot_classes(self,clsname):\n",
    "        ohe = []\n",
    "        for i in self.classes:\n",
    "            if i == clsname:\n",
    "                ohe.append(1)\n",
    "            else:\n",
    "                ohe.append(0)\n",
    "        return ohe\n",
    "\n",
    "    def remove_punctuation(self,text):\n",
    "        return text.translate(self.tbl)\n",
    "\n",
    "\n",
    "    def train_data_with_label(self):\n",
    "        train_text = []\n",
    "        tr_words = []\n",
    "        for cls in self.classes:\n",
    "            train_data_path = self.train_data_dir+ '/' + cls\n",
    "            for i in tqdm(os.listdir(train_data_path)):\n",
    "                if i != '.DS_Store':\n",
    "                    path = os.path.join(train_data_path, i)\n",
    "                    df = pd.read_csv(path,delimiter='\\n')\n",
    "                    dfnp = df.values\n",
    "                    fa = dfnp.flatten('F')\n",
    "                    li = ''.join(fa.tolist())\n",
    "                    dlg = self.remove_punctuation(li)\n",
    "                    dlg_tkn = nltk.word_tokenize(dlg)\n",
    "                    train_text.append([dlg_tkn, [self.classes.index(cls)]])\n",
    "        fd = np.array(train_text)\n",
    "        #np.save('train_text_part.npy', train_text)\n",
    "        #np.save('train_text_wrds_part.npy', tr_words)\n",
    "        return fd\n",
    "\n",
    "    def get_vec_for_new_data(self,path):\n",
    "        test_text = []\n",
    "        df = pd.read_csv(path,delimiter='\\n')\n",
    "        dfnp = df.values\n",
    "        fa = dfnp.flatten('F')\n",
    "        li = ''.join(fa.tolist())\n",
    "        dlg = self.remove_punctuation(li)\n",
    "        dlg_tkn = nltk.word_tokenize(dlg)\n",
    "        test_text.append(dlg_tkn)\n",
    "        return test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = SetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 33.44it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 38.34it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 48.76it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 43.55it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 39.77it/s]\n",
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "fd_text = sd.train_data_with_label();   \n",
    "doc = fd_text[:,0]\n",
    "lb = fd_text[:,1]\n",
    "it = LabeledLineSentence(doc, lb,sd.classes)\n",
    "model = gensim.models.Doc2Vec(size=300, min_count=0, alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(it)\n",
    "model.train(it, epochs=100, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: \n",
      "comedy sciencefic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:00<00:00, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drama comedy\n",
      "comedy drama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.53it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy action\n",
      "action comedy\n",
      "sciencefic: \n",
      "comedy sciencefic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 16.99it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action sciencefic\n",
      "sciencefic action\n",
      "horror: \n",
      "comedy horror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:00<00:00, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencefic action\n",
      "comedy horror\n",
      "horror sciencefic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 17.64it/s]\n",
      " 50%|█████     | 3/6 [00:00<00:00, 29.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horror drama\n",
      "horror sciencefic\n",
      "action: \n",
      "comedy action\n",
      "sciencefic action\n",
      "drama sciencefic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 16.23it/s]\n",
      " 33%|███▎      | 2/6 [00:00<00:00, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drama action\n",
      "sciencefic action\n",
      "drama: \n",
      "drama action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:00<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drama action\n",
      "comedy action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action comedy\n",
      "drama sciencefic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for cls in sd.classes:\n",
    "    test_data_path = sd.test_data_dir+ '/' + cls\n",
    "    print(cls+': ')\n",
    "    for i in tqdm(os.listdir(test_data_path)):\n",
    "        if i != '.DS_Store':\n",
    "            path = os.path.join(test_data_path, i)\n",
    "            td = sd.get_vec_for_new_data(path)\n",
    "            v1 = model.infer_vector(td[0])\n",
    "            print(model.docvecs.most_similar([v1])[0][0], model.docvecs.most_similar([v1])[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 16.69it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.35it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 23.34it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: \n",
      "comedy--->0.1808563768863678\t\t\taction--->0.1149946004152298\n",
      "drama--->0.09855122119188309\t\t\tcomedy--->0.07259517908096313\n",
      "comedy--->0.1209990382194519\t\t\taction--->0.07334736734628677\n",
      "comedy--->0.16781514883041382\t\t\taction--->0.09096236526966095\n",
      "comedy--->0.15442846715450287\t\t\taction--->0.1516314148902893\n",
      "sciencefic: \n",
      "comedy--->0.1113196611404419\t\t\tsciencefic--->0.09614986926317215\n",
      "horror--->0.13223889470100403\t\t\taction--->0.13110636174678802\n",
      "sciencefic--->0.21599653363227844\t\t\taction--->0.1879524141550064\n",
      "horror: \n",
      "comedy--->0.23270288109779358\t\t\thorror--->0.15649518370628357\n",
      "sciencefic--->0.08977347612380981\t\t\tcomedy--->0.05700945109128952\n",
      "comedy--->0.166287362575531\t\t\thorror--->0.16544024646282196\n",
      "horror--->0.9132819175720215\t\t\taction--->0.646616518497467\n",
      "horror--->0.16922016441822052\t\t\tdrama--->0.11785160005092621\n",
      "horror--->0.18545782566070557\t\t\tsciencefic--->0.09572823345661163\n",
      "action: \n",
      "action--->0.0634859949350357\t\t\tcomedy--->0.051785193383693695\n",
      "comedy--->0.14258486032485962\t\t\taction--->0.1284257471561432\n",
      "sciencefic--->0.15443599224090576\t\t\taction--->0.12911322712898254\n",
      "drama--->0.0944405347108841\t\t\tsciencefic--->0.0861605778336525\n",
      "action--->0.11734645813703537\t\t\tdrama--->0.11404213309288025\n",
      "drama: \n",
      "drama--->0.10855119675397873\t\t\taction--->0.09086162596940994\n",
      "drama--->0.1573234647512436\t\t\taction--->0.09686236083507538\n",
      "comedy--->0.0907074585556984\t\t\taction--->0.08725019544363022\n",
      "comedy--->0.048433393239974976\t\t\taction--->0.04508542641997337\n",
      "drama--->0.10013522207736969\t\t\tsciencefic--->0.09958594292402267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_var = ''\n",
    "for cls in sd.classes:\n",
    "    test_data_path = sd.test_data_dir+ '/' + cls\n",
    "    res_var += cls +': \\n'\n",
    "    for i in tqdm(os.listdir(test_data_path)):\n",
    "        if i != '.DS_Store':\n",
    "            path = os.path.join(test_data_path, i)\n",
    "            td = sd.get_vec_for_new_data(path)\n",
    "            v1 = model.infer_vector(td[0])\n",
    "            res_tup = model.docvecs.most_similar([v1])[0]\n",
    "            res_var += res_tup[0] + '--->' + str(res_tup[1]) + '\\t\\t\\t'\n",
    "            res_tup = model.docvecs.most_similar([v1])[1]\n",
    "            res_var += res_tup[0] + '--->' + str(res_tup[1]) + '\\n'\n",
    "print(res_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
