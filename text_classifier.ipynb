{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports for data processing\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import shuffle\n",
    "import string\n",
    "\n",
    "#imports for nural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model\n",
    "\n",
    "class SetData():\n",
    "    def __init__(self):\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.train_data_dir = '/Users/arunprakash/Documents/TensorFlow/MovieData/train'\n",
    "        self.test_data_dir = '/Users/arunprakash/Documents/TensorFlow/MovieData/test'\n",
    "        for _, dirnames, _ in os.walk(self.train_data_dir):\n",
    "            self.num_classes += len(dirnames)\n",
    "            if len(dirnames)>0:\n",
    "                self.classes = dirnames\n",
    "        self.stemmer = LancasterStemmer()\n",
    "        self.tbl = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    def one_hot_classes(self,clsname):\n",
    "        ohe = []\n",
    "        for i in self.classes:\n",
    "            if i == clsname:\n",
    "                ohe.append(1)\n",
    "            else:\n",
    "                ohe.append(0)\n",
    "        return ohe\n",
    "\n",
    "    def remove_punctuation(self,text):\n",
    "        return text.translate(self.tbl)\n",
    "\n",
    "\n",
    "    def train_data_with_label(self):\n",
    "        train_text = []\n",
    "        tr_words = []\n",
    "        for cls in self.classes:\n",
    "            train_data_path = self.train_data_dir+ '/' + cls\n",
    "            for i in tqdm(os.listdir(train_data_path)):\n",
    "                if i != '.DS_Store':\n",
    "                    path = os.path.join(train_data_path, i)\n",
    "                    df = pd.read_csv(path,delimiter='\\n')\n",
    "                    dfnp = df.values\n",
    "                    for dlg in dfnp:\n",
    "                        dlg = self.remove_punctuation(dlg[0])\n",
    "                        dlg_tkn = nltk.word_tokenize(dlg)\n",
    "                        tr_words.extend(dlg_tkn)\n",
    "                        train_text.append((dlg_tkn, self.one_hot_classes(cls)))\n",
    "        tr_words = [self.stemmer.stem(w.lower()) for w in tr_words]\n",
    "        tr_words = sorted(list(set(tr_words)))\n",
    "        shuffle(train_text)\n",
    "        np.save('train_text.npy', train_text)\n",
    "        np.save('train_text_wrds.npy', tr_words)\n",
    "        return train_text, tr_words\n",
    "\n",
    "    def test_data_with_label(self):\n",
    "        test_text = []\n",
    "        tst_words = []\n",
    "        for cls in self.classes:\n",
    "            test_data_path = self.test_data_dir+ '/' + cls\n",
    "            for i in tqdm(os.listdir(test_data_path)):\n",
    "                if i != '.DS_Store':\n",
    "                    path = os.path.join(test_data_path, i)\n",
    "                    df = pd.read_csv(path,delimiter='\\n')\n",
    "                    dfnp = df.values\n",
    "                    for dlg in dfnp:\n",
    "                        dlg = self.remove_punctuation(dlg[0])\n",
    "                        dlg_tkn = nltk.word_tokenize(dlg)\n",
    "                        tst_words.extend(dlg_tkn)\n",
    "                        test_text.append((dlg_tkn, self.one_hot_classes(cls)))\n",
    "        tst_words = [self.stemmer.stem(w.lower()) for w in tst_words]\n",
    "        tst_words = sorted(list(set(tst_words)))\n",
    "        np.save('test_text.npy', test_text)\n",
    "        np.save('test_text_wrds.npy', tst_words)\n",
    "        return test_text,tst_words\n",
    "\n",
    "    def bag_of_words(self, data,wrds,data_type='train'):\n",
    "        model_data = []\n",
    "        batch_data = [] #for saving the bow as batches to overcome memory issue\n",
    "        cnt=1\n",
    "        for i in data:\n",
    "            bow = []\n",
    "            tokenized = i[0]            \n",
    "            tokenized = [self.stemmer.stem(word.lower()) for word in tokenized]\n",
    "            for w in wrds:\n",
    "                bow.append(1) if w in tokenized else bow.append(0)\n",
    "            model_data.append([bow, i[1]])\n",
    "            batch_data.append([bow, i[1]])\n",
    "            if cnt%5000 == 0:\n",
    "                print('processing {}'.format(cnt))\n",
    "                if cnt%50000 == 0:\n",
    "                    batch_res = np.array(batch_data)\n",
    "                    model_name = 'model_data_'+ data_type + str(cnt) +'_part.npy'\n",
    "                    np.save(model_name, batch_res)\n",
    "                    batch_data = []\n",
    "            cnt += 1\n",
    "        res = np.array(model_data)\n",
    "        batch_res = np.array(batch_data)\n",
    "        model_name = 'model_data_'+ data_type +'last_part.npy'\n",
    "        np.save(model_name, batch_res)\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = SetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.75it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  6.58it/s]\n",
      "100%|██████████| 22/22 [00:02<00:00,  7.79it/s]\n",
      "100%|██████████| 21/21 [00:03<00:00,  6.82it/s]\n",
      "100%|██████████| 21/21 [00:03<00:00,  5.75it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.51it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.40it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.07it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 11.27it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Initialize data set\n",
    "train_data, tr_words = sd.train_data_with_label()\n",
    "test_data, tst_words = sd.test_data_with_label()\n",
    "\n",
    "#Load data\n",
    "#train_data = np.load('train_text.npy')\n",
    "#tr_words = np.load('train_text_wrds.npy')\n",
    "#test_data = np.load('test_text.npy')\n",
    "#tst_words = np.load('test_text_wrds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 5000\n",
      "processing 10000\n",
      "processing 15000\n",
      "processing 20000\n",
      "processing 25000\n",
      "processing 30000\n",
      "processing 35000\n",
      "processing 40000\n",
      "processing 45000\n",
      "processing 50000\n",
      "processing 55000\n",
      "processing 60000\n",
      "processing 65000\n",
      "processing 70000\n",
      "processing 75000\n",
      "processing 80000\n",
      "processing 85000\n",
      "processing 90000\n",
      "processing 95000\n",
      "processing 100000\n",
      "processing 105000\n",
      "processing 110000\n",
      "processing 115000\n",
      "processing 120000\n",
      "processing 125000\n",
      "processing 130000\n",
      "processing 135000\n",
      "processing 140000\n",
      "processing 145000\n",
      "processing 150000\n",
      "processing 155000\n",
      "processing 160000\n",
      "processing 165000\n",
      "processing 170000\n"
     ]
    }
   ],
   "source": [
    "training = sd.bag_of_words(train_data,tr_words,'train')\n",
    "#training = np.load('model_data_50000train.npy')\n",
    "\n",
    "training_data = list(training[:, 0])\n",
    "training_label = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 5000\n",
      "processing 10000\n",
      "processing 15000\n",
      "processing 20000\n",
      "processing 25000\n",
      "processing 30000\n",
      "processing 35000\n",
      "processing 40000\n",
      "processing 45000\n"
     ]
    }
   ],
   "source": [
    "testing = sd.bag_of_words(test_data,tr_words,'test')# here also we have to pass the training words\n",
    "#testing = np.load('model_data_test.npy')\n",
    "\n",
    "testing_data = list(testing[:, 0])\n",
    "testing_label = list(testing[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172630/172630 [==============================] - 100s 578us/step - loss: 1.5071 - acc: 0.3331\n",
      "Epoch 2/100\n",
      "172630/172630 [==============================] - 71s 413us/step - loss: 1.3571 - acc: 0.4310\n",
      "Epoch 3/100\n",
      "172630/172630 [==============================] - 71s 413us/step - loss: 1.2421 - acc: 0.4904\n",
      "Epoch 4/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 1.1305 - acc: 0.5433\n",
      "Epoch 5/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 1.0344 - acc: 0.5846\n",
      "Epoch 6/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.9617 - acc: 0.6135\n",
      "Epoch 7/100\n",
      "172630/172630 [==============================] - 71s 411us/step - loss: 0.9025 - acc: 0.6360\n",
      "Epoch 8/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.8586 - acc: 0.6550\n",
      "Epoch 9/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.8244 - acc: 0.6678\n",
      "Epoch 10/100\n",
      "172630/172630 [==============================] - 76s 440us/step - loss: 0.7925 - acc: 0.6813\n",
      "Epoch 11/100\n",
      "172630/172630 [==============================] - 71s 413us/step - loss: 0.7676 - acc: 0.6896\n",
      "Epoch 12/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.7450 - acc: 0.6986\n",
      "Epoch 13/100\n",
      "172630/172630 [==============================] - 72s 419us/step - loss: 0.7280 - acc: 0.7051\n",
      "Epoch 14/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.7095 - acc: 0.7114\n",
      "Epoch 15/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.6956 - acc: 0.7179\n",
      "Epoch 16/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.6777 - acc: 0.7239\n",
      "Epoch 17/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.6670 - acc: 0.7290\n",
      "Epoch 18/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 0.6536 - acc: 0.7330\n",
      "Epoch 19/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.6434 - acc: 0.7373\n",
      "Epoch 20/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.6329 - acc: 0.7406\n",
      "Epoch 21/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 0.6232 - acc: 0.7444\n",
      "Epoch 22/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.6161 - acc: 0.7469\n",
      "Epoch 23/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.6056 - acc: 0.7516\n",
      "Epoch 24/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.6007 - acc: 0.7534\n",
      "Epoch 25/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 0.5926 - acc: 0.7565\n",
      "Epoch 26/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.5863 - acc: 0.7583\n",
      "Epoch 27/100\n",
      "172630/172630 [==============================] - 71s 412us/step - loss: 0.5770 - acc: 0.7628\n",
      "Epoch 28/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.5738 - acc: 0.7644\n",
      "Epoch 29/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.5663 - acc: 0.7662\n",
      "Epoch 30/100\n",
      "172630/172630 [==============================] - 72s 419us/step - loss: 0.5620 - acc: 0.7688\n",
      "Epoch 31/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.5552 - acc: 0.7716\n",
      "Epoch 32/100\n",
      "172630/172630 [==============================] - 71s 413us/step - loss: 0.5507 - acc: 0.7738\n",
      "Epoch 33/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.5441 - acc: 0.7762\n",
      "Epoch 34/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.5429 - acc: 0.7770\n",
      "Epoch 35/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.5346 - acc: 0.7804\n",
      "Epoch 36/100\n",
      "172630/172630 [==============================] - 73s 423us/step - loss: 0.5281 - acc: 0.7835\n",
      "Epoch 37/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 0.5261 - acc: 0.7847\n",
      "Epoch 38/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.5197 - acc: 0.7866\n",
      "Epoch 39/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.5162 - acc: 0.7889\n",
      "Epoch 40/100\n",
      "172630/172630 [==============================] - 72s 414us/step - loss: 0.5096 - acc: 0.7914\n",
      "Epoch 41/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 0.5061 - acc: 0.7932\n",
      "Epoch 42/100\n",
      "172630/172630 [==============================] - 71s 413us/step - loss: 0.5053 - acc: 0.7938\n",
      "Epoch 43/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.4992 - acc: 0.7960\n",
      "Epoch 44/100\n",
      "172630/172630 [==============================] - 71s 413us/step - loss: 0.4982 - acc: 0.7960\n",
      "Epoch 45/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.4908 - acc: 0.7996\n",
      "Epoch 46/100\n",
      "172630/172630 [==============================] - 72s 417us/step - loss: 0.4866 - acc: 0.8011\n",
      "Epoch 47/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.4848 - acc: 0.8031\n",
      "Epoch 48/100\n",
      "172630/172630 [==============================] - 82s 475us/step - loss: 0.4812 - acc: 0.8035\n",
      "Epoch 49/100\n",
      "172630/172630 [==============================] - 72s 420us/step - loss: 0.4780 - acc: 0.8046\n",
      "Epoch 50/100\n",
      "172630/172630 [==============================] - 71s 412us/step - loss: 0.4730 - acc: 0.8070\n",
      "Epoch 51/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.4734 - acc: 0.8071\n",
      "Epoch 52/100\n",
      "172630/172630 [==============================] - 71s 414us/step - loss: 0.4679 - acc: 0.8092\n",
      "Epoch 53/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.4645 - acc: 0.8096\n",
      "Epoch 54/100\n",
      "172630/172630 [==============================] - 73s 424us/step - loss: 0.4632 - acc: 0.8117\n",
      "Epoch 55/100\n",
      "172630/172630 [==============================] - 72s 414us/step - loss: 0.4621 - acc: 0.8119\n",
      "Epoch 56/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.4580 - acc: 0.8132\n",
      "Epoch 57/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.4541 - acc: 0.8136\n",
      "Epoch 58/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.4539 - acc: 0.8151\n",
      "Epoch 59/100\n",
      "172630/172630 [==============================] - 72s 420us/step - loss: 0.4516 - acc: 0.8159\n",
      "Epoch 60/100\n",
      "172630/172630 [==============================] - 73s 420us/step - loss: 0.4459 - acc: 0.8174\n",
      "Epoch 61/100\n",
      "172630/172630 [==============================] - 73s 423us/step - loss: 0.4459 - acc: 0.8179\n",
      "Epoch 62/100\n",
      "172630/172630 [==============================] - 73s 424us/step - loss: 0.4412 - acc: 0.8201\n",
      "Epoch 63/100\n",
      "172630/172630 [==============================] - 75s 432us/step - loss: 0.4403 - acc: 0.8211\n",
      "Epoch 64/100\n",
      "172630/172630 [==============================] - 73s 422us/step - loss: 0.4379 - acc: 0.8207\n",
      "Epoch 65/100\n",
      "172630/172630 [==============================] - 72s 419us/step - loss: 0.4368 - acc: 0.8215\n",
      "Epoch 66/100\n",
      "172630/172630 [==============================] - 72s 420us/step - loss: 0.4362 - acc: 0.8220\n",
      "Epoch 67/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.4355 - acc: 0.8223\n",
      "Epoch 68/100\n",
      "172630/172630 [==============================] - 73s 421us/step - loss: 0.4333 - acc: 0.8229\n",
      "Epoch 69/100\n",
      "172630/172630 [==============================] - 72s 419us/step - loss: 0.4302 - acc: 0.8245\n",
      "Epoch 70/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.4283 - acc: 0.8248\n",
      "Epoch 71/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.4257 - acc: 0.8254\n",
      "Epoch 72/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.4256 - acc: 0.8261\n",
      "Epoch 73/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.4244 - acc: 0.8266\n",
      "Epoch 74/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.4172 - acc: 0.8287\n",
      "Epoch 75/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.4174 - acc: 0.8289\n",
      "Epoch 76/100\n",
      "172630/172630 [==============================] - 72s 418us/step - loss: 0.4186 - acc: 0.8288\n",
      "Epoch 77/100\n",
      "172630/172630 [==============================] - 72s 415us/step - loss: 0.4154 - acc: 0.8293\n",
      "Epoch 78/100\n",
      "172630/172630 [==============================] - 71s 412us/step - loss: 0.4144 - acc: 0.8295\n",
      "Epoch 79/100\n",
      "172630/172630 [==============================] - 72s 416us/step - loss: 0.4129 - acc: 0.8307\n",
      "Epoch 80/100\n",
      "172630/172630 [==============================] - 69s 398us/step - loss: 0.4127 - acc: 0.8307\n",
      "Epoch 81/100\n",
      "172630/172630 [==============================] - 69s 398us/step - loss: 0.4116 - acc: 0.8308\n",
      "Epoch 82/100\n",
      "172630/172630 [==============================] - 68s 396us/step - loss: 0.4085 - acc: 0.8320\n",
      "Epoch 83/100\n",
      "172630/172630 [==============================] - 69s 399us/step - loss: 0.4045 - acc: 0.8334\n",
      "Epoch 84/100\n",
      "172630/172630 [==============================] - 69s 398us/step - loss: 0.4042 - acc: 0.8337\n",
      "Epoch 85/100\n",
      "172630/172630 [==============================] - 69s 397us/step - loss: 0.4031 - acc: 0.8343\n",
      "Epoch 86/100\n",
      "172630/172630 [==============================] - 68s 396us/step - loss: 0.4037 - acc: 0.8337\n",
      "Epoch 87/100\n",
      "172630/172630 [==============================] - 68s 396us/step - loss: 0.4006 - acc: 0.8352\n",
      "Epoch 88/100\n",
      "172630/172630 [==============================] - 68s 395us/step - loss: 0.4044 - acc: 0.8336\n",
      "Epoch 89/100\n",
      "172630/172630 [==============================] - 69s 398us/step - loss: 0.4000 - acc: 0.8353\n",
      "Epoch 90/100\n",
      "172630/172630 [==============================] - 68s 396us/step - loss: 0.3963 - acc: 0.8361\n",
      "Epoch 91/100\n",
      "172630/172630 [==============================] - 68s 396us/step - loss: 0.3986 - acc: 0.8354\n",
      "Epoch 92/100\n",
      "172630/172630 [==============================] - 68s 396us/step - loss: 0.3950 - acc: 0.8365\n",
      "Epoch 93/100\n",
      "172630/172630 [==============================] - 69s 397us/step - loss: 0.3968 - acc: 0.8363\n",
      "Epoch 94/100\n",
      "172630/172630 [==============================] - 68s 397us/step - loss: 0.3937 - acc: 0.8365\n",
      "Epoch 95/100\n",
      "172630/172630 [==============================] - 69s 397us/step - loss: 0.3920 - acc: 0.8380\n",
      "Epoch 96/100\n",
      "172630/172630 [==============================] - 69s 398us/step - loss: 0.3884 - acc: 0.8394\n",
      "Epoch 97/100\n",
      "172630/172630 [==============================] - 69s 399us/step - loss: 0.3863 - acc: 0.8398\n",
      "Epoch 98/100\n",
      "172630/172630 [==============================] - 69s 398us/step - loss: 0.3876 - acc: 0.8392\n",
      "Epoch 99/100\n",
      "172630/172630 [==============================] - 69s 402us/step - loss: 0.3892 - acc: 0.8395\n",
      "Epoch 100/100\n",
      "172630/172630 [==============================] - 69s 401us/step - loss: 0.3892 - acc: 0.8393\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15685)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2007808   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 2,034,821\n",
      "Trainable params: 2,034,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "#ANN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer(input_shape=[len(training_data[0])]))#keras will internally add batch dimention\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(sd.num_classes,activation='softmax'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x=np.array(training_data),y=np.array(training_label),epochs=100,batch_size=200)\n",
    "model.summary()\n",
    "\n",
    "model.save('my_classify_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(np.array(testing_data), np.array(testing_label), batch_size=100)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import shuffle\n",
    "import string\n",
    "\n",
    "#imports for nural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('my_classify_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = SetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a =np.array(testing_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = sd.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.reshape(15685,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drama\n"
     ]
    }
   ],
   "source": [
    "print(categories[np.argmax(model.predict(np.array(testing_data[40000:40001])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(np.array(training_data[25000:25001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15685)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(testing_data[40000:40001]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_label[39999:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data[40000:40001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172630"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comedy', 'sciencefic', 'horror', 'action', 'drama']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow_for_new_sub(path):\n",
    "    test_text = []\n",
    "    df = pd.read_csv(path,delimiter='\\n')\n",
    "    dfnp = df.values\n",
    "    for dlg in dfnp:\n",
    "        dlg = sd.remove_punctuation(dlg[0])\n",
    "        dlg_tkn = nltk.word_tokenize(dlg)\n",
    "        test_text.append((dlg_tkn,[0]))\n",
    "    return test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td = get_bow_for_new_sub('/Users/arunprakash/Documents/TensorFlow/MovieData/test/action/The.Rock.1996.720p.DVD9.BluRay.x264-SEPTiC_eng.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td_bow = sd.bag_of_words(td,tr_words,data_type='my_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdb = list(td_bow[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2630"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 744 sciencefic: 458 horror: 359 action: 487 drama: 582\n"
     ]
    }
   ],
   "source": [
    "comedy = 0\n",
    "sciencefic = 0\n",
    "drama = 0\n",
    "action = 0\n",
    "horror = 0\n",
    "for i in range(len(tdb)):\n",
    "    pred = categories[np.argmax(model.predict(np.array(tdb[i:i+1])))]\n",
    "    if pred == 'comedy':\n",
    "        comedy += 1\n",
    "    elif pred == 'sciencefic':\n",
    "        sciencefic += 1\n",
    "    elif pred == 'horror':\n",
    "        horror += 1\n",
    "    elif pred == 'action':\n",
    "        action += 1\n",
    "    elif pred == 'drama':\n",
    "        drama += 1\n",
    "print('comedy: '+ str(comedy) + ' sciencefic: '+ str(sciencefic) + ' horror: '+ str(horror) + ' action: '+ str(action) + ' drama: '+ str(drama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tfdeeplearningv1.4/lib/python3.5/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      " 17%|█▋        | 1/6 [00:15<01:17, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 1074 sciencefic: 433 horror: 332 action: 336 drama: 387\n",
      "comedy: 0 sciencefic: 0 horror: 0 action: 0 drama: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:26<00:26,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 642 sciencefic: 277 horror: 281 action: 246 drama: 392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:42<00:21, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 972 sciencefic: 345 horror: 449 action: 363 drama: 529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [01:04<00:12, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 1488 sciencefic: 413 horror: 537 action: 489 drama: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:28<00:00, 14.83s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 1546 sciencefic: 735 horror: 564 action: 580 drama: 660\n",
      "sciencefic: \n",
      "comedy: 0 sciencefic: 0 horror: 0 action: 0 drama: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [03:46<03:46, 113.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 287 sciencefic: 152 horror: 226 action: 131 drama: 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [04:03<01:21, 81.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 699 sciencefic: 411 horror: 398 action: 359 drama: 806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [04:12<00:00, 63.10s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 328 sciencefic: 287 horror: 257 action: 252 drama: 272\n",
      "horror: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 1/7 [00:08<00:52,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 584 sciencefic: 148 horror: 230 action: 195 drama: 193\n",
      "comedy: 0 sciencefic: 0 horror: 0 action: 0 drama: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 3/7 [00:19<00:25,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 444 sciencefic: 257 horror: 299 action: 201 drama: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 4/7 [00:25<00:18,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 417 sciencefic: 109 horror: 187 action: 131 drama: 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 5/7 [00:36<00:14,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 191 sciencefic: 43 horror: 1557 action: 11 drama: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 6/7 [00:48<00:08,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 501 sciencefic: 244 horror: 414 action: 267 drama: 454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:50<00:00,  7.19s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 67 sciencefic: 42 horror: 124 action: 36 drama: 74\n",
      "action: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:05<00:29,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 323 sciencefic: 139 horror: 169 action: 136 drama: 223\n",
      "comedy: 0 sciencefic: 0 horror: 0 action: 0 drama: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:13<00:13,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 425 sciencefic: 184 horror: 203 action: 221 drama: 272\n",
      "comedy: 1 sciencefic: 0 horror: 1 action: 0 drama: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:29<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 744 sciencefic: 458 horror: 359 action: 487 drama: 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:33<00:00,  5.66s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 215 sciencefic: 114 horror: 144 action: 125 drama: 166\n",
      "drama: \n",
      "comedy: 0 sciencefic: 0 horror: 0 action: 0 drama: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:09<00:18,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 348 sciencefic: 249 horror: 256 action: 249 drama: 494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:22<00:22,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 498 sciencefic: 354 horror: 308 action: 321 drama: 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:37<00:18,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 839 sciencefic: 373 horror: 447 action: 433 drama: 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:54<00:10, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 819 sciencefic: 340 horror: 354 action: 404 drama: 762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [01:10<00:00, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy: 691 sciencefic: 461 horror: 422 action: 417 drama: 764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for cls in sd.classes:\n",
    "    test_data_path = sd.test_data_dir+ '/' + cls\n",
    "    print(cls+': ')\n",
    "    for i in tqdm(os.listdir(test_data_path)):\n",
    "        comedy = 0\n",
    "        sciencefic = 0\n",
    "        drama = 0\n",
    "        action = 0\n",
    "        horror = 0\n",
    "        if i != '.DS_Store':\n",
    "            #path = os.path.join(test_data_path, i)\n",
    "            td = get_bow_for_new_sub(os.path.join(test_data_path, i))\n",
    "            td_bow = sd.bag_of_words(td,tr_words,data_type='my_test')\n",
    "            tdb = list(td_bow[:,0])\n",
    "            for i in range(len(tdb)):\n",
    "                pred = categories[np.argmax(model.predict(np.array(tdb[i:i+1])))]\n",
    "                if pred == 'comedy':\n",
    "                    comedy += 1\n",
    "                elif pred == 'sciencefic':\n",
    "                    sciencefic += 1\n",
    "                elif pred == 'horror':\n",
    "                    horror += 1\n",
    "                elif pred == 'action':\n",
    "                    action += 1\n",
    "                elif pred == 'drama':\n",
    "                    drama += 1\n",
    "        print('comedy: '+ str(comedy) + ' sciencefic: '+ str(sciencefic) + ' horror: '+ str(horror) + ' action: '+ str(action) + ' drama: '+ str(drama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d76a646c9c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
